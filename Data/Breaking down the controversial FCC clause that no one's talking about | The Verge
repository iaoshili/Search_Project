{"Main text":"Share on Facebook Tweet Share Pin Share\nThis week, the FCC released the new rules for internet service providers , 400 pages of rules stemming from the decision last month to adopt Title II authority . The most important news from the decision has been net neutrality \u2014 it's now illegal to prioritize certain kinds of web traffic for money \u2014 but there's a separate, more obscure provision that had raised the alarm for many network engineers, and the new rules put it in a radically different light.\nServices will be forced to compete to see who can run the cleanest network\nThe center of the controversy is packet loss, the basic noise that comes when networks lose track of data in transit. The new rules will force internet providers to measure and publish data on packet loss across their networks. The idea behind the move is simple: if Comcast lists less packet loss than Optimum, say, you might be more inclined to use their service, anticipating fewer hiccups in your data feed. Internet protocols are built to withstand packet loss, the same way a car won\u2019t stop because of a few bumps in the pavement, but extremely high loss can still slow or stop streaming and browsing. If consumers flock to services with less packet loss, services will be forced to compete to see who can run the cleanest network, potentially leading to better service across the board. Packet loss is also a good measure for the kind of service decline you'd see if you were stuck in an internet slow lane, so groups from the EFF to the AARP had called for better reporting on the metric.\nWhile it seems like a harmless nudge, the idea of competing for lower packet loss struck many engineers as potentially catastrophic. The problem is, packet loss isn't the best measure of a good network, and a system designed entirely to minimize packet loss might simply add more buffering, cratering latency and incentivizing slower networks overall. That led the National Cable and Television Association (already outspoken opponents of Title II) to label the packet loss provision as \"a new rule that could degrade internet performance.\"\n\"An angel in disguise\"\nBut now that we've got the full text of the FCC rules, the situation seems to have gone up in smoke. The FCC's initial press releases had stoked fears by only mentioning packet loss, but the full regulations describe packet loss metrics being integrated into existing requirements on average latency and bandwidth. That\u2019s a big difference: If packet loss is the only metric, ISPs might rush to get the number as low as possible \u2014 but when it\u2019s printed alongside bandwidth and latency, the incentives are much less perverse. Customers may not understand all the new metrics, but ISPs will be charged with balancing the three numbers, which should fend off the nightmare scenario the NCTA was worried about. As a result, many early critics have already changed their tune. Nicholas Weaver, who criticized the measure after the early FCC proposals leaked, yesterday called the packet loss requirement \"an angel in disguise\" for adding crucial new information without upsetting the balance between connection speed and quality.\nIt\u2019s still tricky to say exactly the effect the new packet loss requirements will have. Customers may not be able to perfectly balance the new information, and ISPs could still overreact to the new requirements. Crucially, they\u2019ll still have control over their own networks, but it\u2019s still conceivable that network operators will overreact to the new provisions, even if they\u2019ll have less reason to. Most of all, the controversy shows how delicate network regulation is, and how easily a minor provision can snowball into an unforeseen disaster. Luckily, in this case the unintended consequences seem to have been a false alarm.\nVerge Videos: The history of net neutrality\n","Title":"Breaking down the controversial FCC clause that no one's talking about | The Verge","Tags":["policy","report","us-world"]}