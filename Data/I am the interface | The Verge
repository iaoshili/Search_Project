{"Main text":"36 Comments\nAsteroids barreled towards Earth from every direction, and my only defense was my eyes. I looked at one asteroid. I looked at another. Boom. Boom. Lasers from Earth had blown them into smithereens. And I didn\u2019t lift a finger.\nAt CES 2014, a dozen or more companies are vying to track your arms, legs, and even your eyeballs. This demo was from Tobii , an eye-tracking technology company from Stockholm that wants to change the way we read, drive, and game. The company makes Kinect-like sensors, which each have two or three cameras and sit below your laptop\u2019s screen. After two minutes of calibration (which involved staring at several dots as they darted around), I could pinpoint any spot on the screen with my eyes.\nTobii\u2019s \u201CEye Asteroids\u201D demo was a blast, and was easier to control with my eyes than with an analog stick. Navigating Windows 8 with my eyes, on the other hand, wasn\u2019t as much fun. In order to select an app, I needed to not only look at an app, but then press a keyboard button with my hand. I quickly learned that eye tracking doesn\u2019t solve everything. In fact, the dozens of eye- and gesture-tracking companies at CES seem to be creating more problems than they\u2019re solving.\nBut a few get it right. Now please, keep your hands and feet inside the car.\nFor better or worse, we\u2019re still under the spell of Tom Cruise in Minority Report, who effortlessly moved through time and space solving crime using only his hands. Nearly every interface company I spoke with at CES mentioned the film \u2014 its iconic interface inspiring everything from car dashboards to operating systems you control with your fingers sprawled out longingly in midair. But hoping for Minority Report interfaces is a bit of a fantasy, says Dale Herigstad, whom Steven Spielberg hired to help design the film\u2019s futuristic interfaces.\nNaturally, Herigstad is showing off his own product at CES this year. It\u2019s called InAir , a TV interface that layers information like sports scores and IMDB over what you\u2019re watching. One would expect Herigstad\u2019s interface to be operated with your hands, but he chose phones instead. His team started with Kinect, but realized that people are most often parked on the couch while watching TV and don\u2019t want to wave their hands to change the channel each and every time. \"If you look at what Tom Cruise is doing, it\u2019s rather technical,\" he says. \"A TV audience sometimes wants to have no interface at all.\"\nComing from the guy who helped design the PreCrime division\u2019s iconic computer interface, I was a little disappointed. But he has a point: sometimes Minority Report holds us back more than it helps. I tried out HAL.TV , a new gestural interface for TVs that lets you wave your hand to open apps, switch channels, and turn up the volume. It worked poorly and I was exhausted after waving my hand over and over again to change the volume and switch channels. Not every device needs its own glorified gestural interface.\nBut some do. One of the Oculus Rift\u2019s most annoying limitations is that it simulates your vision, but not your sense of touch. When you strap SoftKinetic\u2019s DepthSense camera kit to your Oculus Rift, however, manipulating 3D objects in space becomes a reality. I tested out SoftKinetic with a very rudimentary building-block program that let me grab blocks (with both hands at once, even!) and stack them in a 3D grid world. I felt like Johnny Mnemonic moving effortlessly through cyberspace. I couldn\u2019t feel the blocks I was touching, of course, but I think that\u2019s an acceptable limitation \u2014 for now. Within a couple years, this is how we\u2019ll all play Minecraft.\nWithout a multicamera sensor on your head or on your computer, gesture and movement recognition is much more difficult. But always-on cameras suck power and aren\u2019t feasible for mobile devices. While other companies fooled around with cameras and parallax, San Francisco\u2019s Elliptic Labs has been busy tweaking its ultrasonic wave technology. Like comic hero Daredevil, Elliptic Labs\u2019 tiny speakers constantly fire off ultrasonic waves that bounce off your hand, then bounce back into your phone, tablet, or computer mic. Then, the company\u2019s software determines the location of your hand (or face) in space. You can scroll, switch apps, change songs, and answer calls with a wave of your hand. Most incredibly, the technology works even if your hand isn\u2019t hovering over your screen like you might do to answer a call on a Galaxy S4. You can hold your hand a few inches to the side (or below) your phone, and gesture just as well. I was shocked the first time I tried it and it actually worked.\nWithin a couple years, this is how we\u2019ll all play  'Minecraft'\nCEO Laila Danielsen says that the company already has deals with several OEMs in the works, which means phones and tablets could have more accurate versions of Samsung\u2019s limited-range Air Command as soon as next year. The toughest part will be explaining to consumers why they want to wave at their phone. Using your hands to quickly answer calls and change songs in the car seems like the most valid use case. Tobii also targets the car for its eye-tracking \u2014 an opportunity it sees as perhaps its most lucrative. With the company\u2019s sensors mounted above the steering wheel in every car, you can signify that you want to make a call without taking your eyes off the road. The company\u2019s demo also lets you change radio stations with your eyes and lets you know if you\u2019re drifting off. A few car companies have already implemented such technology, but Tobii plans to take it mainstream.\nOnce the iPhone launched, Herigstad says, people began to understand literal, direct manipulation with digital objects. It may have taken a few years, but today, even toddlers seem to understand the mechanics of pinching and zooming on an iPad. In order to push gesture-based 3D interfaces even further into the mainstream, Intel plans to integrate its RealSense 3D technology into a growing number of its partners\u2019 laptops, tablets, and desktops within the coming months. RealSense works very much like Kinect, giving your computer not one but two eyes. This means your computer will be able to detect 3D objects like your hands or an object you want to scan into your computer. I demoed a whimsical music-making app that let me play virtual guitar, piano, and drums by moving my hands in the air. I could grab different instruments at will, combining them or tossing them off-screen. Intel\u2019s demos weren\u2019t very impressive, but they herald a future where every computer has a 3D camera inside it.\nJust because we\u2019re obsessed with 'Minority Report' doesn\u2019t mean that\u2019s how our future should look\nAt the end of the day, my hands and eyes were tired. Full-on gestural, eye-tracking interfaces are exhausting because they demand that you interact with your computers and TVs, when ordinarily I\u2019d be lazily slouched into a chair with a remote or Xbox 360 controller in my hand. In testing many of the hottest interactive technologies, I discovered that these devices are the future \u2014 not the future of everything, but the future of driving safety and the future of gaming. And perhaps they're the future of other things, as soon as appropriate gesture-based interfaces are developed. These gestures might help doctors move an x-ray from one display to another without using their hands, or simply help you wave to close your garage door. Tobii already has 15,000 handicapped people around the world using its products to type messages to family and friends using only their eyes.\nJust because we\u2019re obsessed with Minority Report doesn\u2019t mean that\u2019s how our future should look. PreCrime\u2019s sensational computer interface is almost impossibly technical \u2014 and wouldn\u2019t be useful for most games, jobs, and tools. But Spielberg\u2019s effects crew was right about a few things \u2014 interfaces that didn\u2019t just look good but also felt right. \"We have grown up with media that has flattened 3D and we have gotten lazy,\" Herigstad says. \"Our eyes have gotten fat, and our eyes don\u2019t have to refocus things.\" Minority Report, and more recently Iron Man, introduced the masses to direct three-dimensional manipulation , which will require a bit more work on the part of the user \u2014 but it will yield some pretty amazing and useful technology. And you won\u2019t even have to be Tony Stark or John Anderton to use it.\n","Title":"I am the interface | The Verge","Tags":["gaming","culture","hands--on","report","videos","design","tech","ces-2014\",\"Story Streams\"","\"5056023"]}